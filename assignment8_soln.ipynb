{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ECON0127: Statistical Learning for Public Policy\n",
    "# Assignment 8\n",
    "\n",
    "**Instructions**: This assignment is voluntary and does not count towards your final assessment. It will be discussed in your tutorial session on March 20. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **General Feedback**: \n",
    "\n",
    "To be updated - sorry!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Part 0: Load and Explore Data**\n",
    "\n",
    "In this assignment, we continue to use the text data from the **10-K reports** filed by publicly-traded firms in the U.S. in 2019. Remember in Assignment 4, we build boosted trees to predict the sector membership based on word count. Now we have learned word embeddings and large language models, would it help us predict the sector membership better?\n",
    "\n",
    "The raw data of 10-K reports has a total of 1,744,131 sentences for 4,033 firms. However, to decrease training time, we will work with a subset of firms 22 firms from 4 different economic sectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from transformers import BertTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6798, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>cik</th>\n",
       "      <th>year</th>\n",
       "      <th>sent_no</th>\n",
       "      <th>sent_id</th>\n",
       "      <th>naics2</th>\n",
       "      <th>naics2_name</th>\n",
       "      <th>sentence_len</th>\n",
       "      <th>keep_sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The following discussion sets forth the materi...</td>\n",
       "      <td>19617</td>\n",
       "      <td>2019</td>\n",
       "      <td>0</td>\n",
       "      <td>19617_0</td>\n",
       "      <td>52</td>\n",
       "      <td>Finance and Insurance</td>\n",
       "      <td>18</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Readers should not consider any descriptions o...</td>\n",
       "      <td>19617</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>19617_1</td>\n",
       "      <td>52</td>\n",
       "      <td>Finance and Insurance</td>\n",
       "      <td>23</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Any of the risk factors discussed below could ...</td>\n",
       "      <td>19617</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>19617_2</td>\n",
       "      <td>52</td>\n",
       "      <td>Finance and Insurance</td>\n",
       "      <td>53</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JPMorgan Chase's businesses are highly regulat...</td>\n",
       "      <td>19617</td>\n",
       "      <td>2019</td>\n",
       "      <td>4</td>\n",
       "      <td>19617_4</td>\n",
       "      <td>52</td>\n",
       "      <td>Finance and Insurance</td>\n",
       "      <td>25</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JPMorgan Chase is a financial services firm wi...</td>\n",
       "      <td>19617</td>\n",
       "      <td>2019</td>\n",
       "      <td>5</td>\n",
       "      <td>19617_5</td>\n",
       "      <td>52</td>\n",
       "      <td>Finance and Insurance</td>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentences    cik  year  sent_no  \\\n",
       "0  The following discussion sets forth the materi...  19617  2019        0   \n",
       "1  Readers should not consider any descriptions o...  19617  2019        1   \n",
       "2  Any of the risk factors discussed below could ...  19617  2019        2   \n",
       "3  JPMorgan Chase's businesses are highly regulat...  19617  2019        4   \n",
       "4  JPMorgan Chase is a financial services firm wi...  19617  2019        5   \n",
       "\n",
       "   sent_id  naics2            naics2_name  sentence_len  keep_sent  \n",
       "0  19617_0      52  Finance and Insurance            18       True  \n",
       "1  19617_1      52  Finance and Insurance            23       True  \n",
       "2  19617_2      52  Finance and Insurance            53       True  \n",
       "3  19617_4      52  Finance and Insurance            25       True  \n",
       "4  19617_5      52  Finance and Insurance            10       True  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data\n",
    "file_id = \"1eQB8rwSklyVD3u8sZImFII74b7jBeUIL\"\n",
    "df = pd.read_parquet(f\"https://drive.google.com/uc?export=download&id={file_id}&authuser=0&export=download\")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6798, 9)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'If spam and fake accounts increase on Twitter, this could hurt our reputation for delivering relevant content or reduce user growth rate and user engagement and result in continuing operational cost to us.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read some observations\n",
    "df.loc[1001, \"sentences\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "naics2_name\n",
       "Finance and Insurance    1824\n",
       "Information              2341\n",
       "Manufacturing            1588\n",
       "Retail Trade             1045\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# explore the economic sectors covered by the data\n",
    "df.groupby(\"naics2_name\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many firms are in the data?\n",
    "df[\"cik\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sector_labels\n",
       "2    2341\n",
       "3    1824\n",
       "0    1588\n",
       "1    1045\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add labels as Y\n",
    "sector_labels = df[\"naics2\"].astype(\"category\").cat.codes\n",
    "df[\"sector_labels\"] = sector_labels\n",
    "df[\"sector_labels\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Part 1: Bert Base Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1**. Tokenize the text data from 10-K filings using the pre-trained model `bert-base-uncased`. Randomly select one sentence, print out the original text and the tokenized version.\n",
    "\n",
    "*Instructions*: \n",
    "\n",
    "- We initialize a tokenizer using `BertTokenizer.from_pretrained()` and put in the model name.\n",
    "- In Stephen's lecture, each time we tokenize a sentence. Alternatively, you can do batch-tokenization by converting the values from the column `sentences` into a list and feed into the tokenizer at once.\n",
    "- Specify several hyperparameters: `truncation` (whether or not to truncate the sequences longer than specified length), `max_length` (maximum number of tokens per sequence), `padding` (pad all sequences to the same size), `return_tensors` (data type of results)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the sentences\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer\n",
    "\n",
    "encoded_sentences = tokenizer(list(df[\"sentences\"].values),     # list of sequences we want to tokenize\n",
    "                              truncation=True,                  # truncate sequences longer than specified length\n",
    "                              max_length=5,                    # maximum number of tokens per sequence （降低计算成本）\n",
    "                              padding=\"max_length\",             # pad all sequences to the same size\n",
    "                              return_tensors='pt'               # data type of results\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence:\n",
      "Any of these events could negatively impact Cat Financial's business, as well as our and Cat Financial's results of operations and financial condition.\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "Tokens:\n",
      "['[CLS]', 'any', 'of', 'these', '[SEP]']\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "Tokens IDs:\n",
      "tensor([ 101, 2151, 1997, 2122,  102])\n"
     ]
    }
   ],
   "source": [
    "# examine BERT's tokenization in detail for a random sentence\n",
    "i = np.random.randint(0, len(df)) # 随机选择一句话\n",
    "print(\"Original sentence:\")\n",
    "print(df.loc[i, \"sentences\"]) # 打印原始句子\n",
    "print(\"\\n------------------------------------------\\n\")\n",
    "print(\"Tokens:\")\n",
    "temp_tokens = encoded_sentences[\"input_ids\"][i] # 取出该句子的 token id 序列\n",
    "print(tokenizer.convert_ids_to_tokens(temp_tokens)) # 打印token id 序列对应的 token\n",
    "print(\"\\n------------------------------------------\\n\")\n",
    "print(\"Tokens IDs:\")\n",
    "print(temp_tokens)\n",
    "#  Tokens IDs 是 BERT 分词器（tokenizer）将原始句子转换为模型输入的数字序列。\n",
    "#  这些数字是每个 token（词或子词）在 BERT 词表（vocabulary）中的唯一编号。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>[CLS] 的 embedding 是 BERT 句子级分类任务的“句子表示”。\n",
    "[SEP] 用于分隔句子或标记句子结束。</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2**. Load the model `bert-base-uncased` and get sequence embedding by passing the tokenized sentences to the model.\n",
    "\n",
    "*Instructions*: \n",
    "\n",
    "- Intialize the model using `AutoModel.from_pretrained()`.\n",
    "- Pass the tokens through the model. \n",
    "- All codes you can follow from Stephen's lecture.\n",
    "- It may take a while to run this step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT model\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\",\n",
    "                                  output_hidden_states=True,\n",
    "                                  output_attentions=True,\n",
    "                                  attn_implementation=\"eager\"\n",
    "                                  )\n",
    "# Pass through model\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3**. Extract the embeddings for predicting the sector membership in the following two ways: (i) use the average of the token embeddings as taught in Stephen's lecture, or (ii) use embedding for the `CLS` token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6798, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract token embeddings (excluding the last hidden state)\n",
    "token_embeddings = model_output.last_hidden_state  # 这是BERT输出的每个token的向量表示，Shape: (batch_size, sequence_length, hidden_size)\n",
    "\n",
    "# Extract attention mask to avoid averaging over padding tokens\n",
    "attention_mask = encoded_sentences[\"attention_mask\"]  # 去掉[pad]的真实token，Shape: (batch_size, sequence_length)\n",
    "\n",
    "# Expand attention mask to match embedding dimensions\n",
    "attention_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "\n",
    "# Compute mean embedding (excluding padding tokens)\n",
    "sum_embeddings = torch.sum(token_embeddings * attention_mask_expanded, dim=1)\n",
    "sum_mask = attention_mask_expanded.sum(dim=1)\n",
    "mean_embeddings = sum_embeddings / sum_mask  # Shape: (batch_size, hidden_size)\n",
    "mean_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6798, 768)\n"
     ]
    }
   ],
   "source": [
    "# Extract CLS embeddings\n",
    "cls_embeddings = model_output.last_hidden_state[:, 0, :].numpy() # 选取所有句子，每个句子的第一个 token（也就是 [CLS] ），该 token 的所有 embedding 维度。\n",
    "print(cls_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4**. Now we build multinomial logistic regression models to predict the sector membership.\n",
    "\n",
    "*Instructions*:\n",
    "\n",
    "- Create the sector label as numbers 0 to 3, corresponding to the sector classification based on `naics2`.\n",
    "- Train **two** multinomial logistic regression models:\n",
    "  1. One using mean embeddings as input features X.\n",
    "  2. One using CLS embeddings as input features X.\n",
    "- Evaluate both models using `classification_report` to compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sector_labels\n",
       "2    2341\n",
       "3    1824\n",
       "0    1588\n",
       "1    1045\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add labels as Y\n",
    "sector_labels = df[\"naics2\"].astype(\"category\").cat.codes\n",
    "df[\"sector_labels\"] = sector_labels\n",
    "df[\"sector_labels\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>你不需要把 mean_embeddings 合并到 df，因为顺序已经对齐：\n",
    "\n",
    "mean_embeddings 的每一行，顺序和你输入 BERT 的句子顺序一致（即 df[\"sentences\"] 的顺序）</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.48      0.53      1588\n",
      "           1       0.56      0.33      0.42      1045\n",
      "           2       0.53      0.72      0.61      2341\n",
      "           3       0.59      0.56      0.57      1824\n",
      "\n",
      "    accuracy                           0.56      6798\n",
      "   macro avg       0.57      0.52      0.53      6798\n",
      "weighted avg       0.57      0.56      0.55      6798\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert embeddings to NumPy for ML models\n",
    "X = mean_embeddings.cpu().numpy() # 把PyTorch张量转为NumPy数组，方便sklearn使用\n",
    "y = np.array(sector_labels) # 把标签转为NumPy数组\n",
    "\n",
    "# Train classification models\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X, y)\n",
    "\n",
    "# Predictions\n",
    "log_reg_preds = log_reg.predict(X)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Logistic Regression Performance:\")\n",
    "print(classification_report(y, log_reg_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Performance (CLS embeddings):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.43      0.49      1588\n",
      "           1       0.59      0.32      0.42      1045\n",
      "           2       0.51      0.71      0.59      2341\n",
      "           3       0.57      0.54      0.56      1824\n",
      "\n",
      "    accuracy                           0.54      6798\n",
      "   macro avg       0.56      0.50      0.51      6798\n",
      "weighted avg       0.55      0.54      0.53      6798\n",
      "\n",
      "Accuracy (mean embeddings): 0.5613415710503089\n",
      "Accuracy (CLS embeddings): 0.5406001765225066\n"
     ]
    }
   ],
   "source": [
    "# Use CLS embeddings to train models\n",
    "X_cls = cls_embeddings\n",
    "log_reg.fit(X_cls, y)\n",
    "\n",
    "# Predictions\n",
    "log_reg_preds_cls = log_reg.predict(X_cls)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Logistic Regression Performance (CLS embeddings):\")\n",
    "print(classification_report(y, log_reg_preds_cls))\n",
    "\n",
    "# Accuracy\n",
    "print(\"Accuracy (mean embeddings):\", accuracy_score(y, log_reg_preds))\n",
    "print(\"Accuracy (CLS embeddings):\", accuracy_score(y, log_reg_preds_cls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5. [Bonus]** Compare the above models with boosted trees with word counts as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.83      0.89      1588\n",
      "           1       0.97      0.73      0.84      1045\n",
      "           2       0.79      0.96      0.87      2341\n",
      "           3       0.91      0.88      0.90      1824\n",
      "\n",
      "    accuracy                           0.87      6798\n",
      "   macro avg       0.91      0.85      0.87      6798\n",
      "weighted avg       0.89      0.87      0.87      6798\n",
      "\n",
      "Accuracy: 0.8746690203000883\n",
      "XGBoost Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.61      0.68       309\n",
      "           1       0.71      0.48      0.57       219\n",
      "           2       0.61      0.80      0.69       441\n",
      "           3       0.76      0.76      0.76       391\n",
      "\n",
      "    accuracy                           0.69      1360\n",
      "   macro avg       0.71      0.66      0.68      1360\n",
      "weighted avg       0.71      0.69      0.69      1360\n",
      "\n",
      "Accuracy: 0.6919117647058823\n"
     ]
    }
   ],
   "source": [
    "# Build a XGBoost model with word counts as input\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# 这段代码的作用是用词袋模型（CountVectorizer）+ XGBoost 分类器，对原始句子进行分类，并评估模型效果。详细解释如下：\n",
    "# 创建一个流水线，先用 CountVectorizer() 把句子转为词频特征（词袋模型），再用 XGBClassifier() 做分类。\n",
    "pipe = make_pipeline(CountVectorizer(), XGBClassifier()) \n",
    "\n",
    "# Train model without train-test split\n",
    "pipe.fit(df[\"sentences\"], df[\"sector_labels\"])\n",
    "\n",
    "# Predictions\n",
    "xgb_preds = pipe.predict(df[\"sentences\"])\n",
    "\n",
    "# Evaluation\n",
    "print(\"XGBoost Performance:\")\n",
    "print(classification_report(y, xgb_preds))\n",
    "print(\"Accuracy:\", accuracy_score(y, xgb_preds))\n",
    "\n",
    "# 试试样本外测试\n",
    "# Train model with train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"sentences\"], y, test_size=0.2, random_state=42)\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "xgb_preds = pipe.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"XGBoost Performance:\")\n",
    "print(classification_report(y_test, xgb_preds))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, xgb_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Part 2: Domain-specific Models**\n",
    "\n",
    "So far, we have used the base version of BERT to generate features. However, one concern with this approach is that the language in the training data used for base BERT (i.e. Wikipedia and Books) migth be very different from the language in 10-K reports. In order to alleviate this concern, we will use a different version of the model trained on 260,773 10-K filings from 1993-2019.\n",
    "\n",
    "These family of models are called `SEC-BERT` and were developed by the Natural Language Processing Group at the Athens University of Economics and Business. \n",
    "\n",
    "**Q6.** Repeat the process in part 1 but tokenize and embed with model `nlpaueb/sec-bert-base` instead. Compare the modeling performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec_tokenizer = BertTokenizer.from_pretrained(\"nlpaueb/sec-bert-base\")\n",
    "\n",
    "sec_sentences = sec_tokenizer(list(df[\"sentences\"].values),     # list of sequences we want to tokenize\n",
    "                              truncation=True,                  # truncate sequences longer than specified length\n",
    "                              max_length=60,                    # maximum number of tokens per sequence\n",
    "                              padding=\"max_length\",             # pad all sequences to the same size\n",
    "                              return_tensors='pt'               # data type of results\n",
    "                              )\n",
    "\n",
    "sec_model = AutoModel.from_pretrained(\"nlpaueb/sec-bert-base\",\n",
    "                                  output_hidden_states=True,\n",
    "                                  output_attentions=True,\n",
    "                                  attn_implementation=\"eager\"\n",
    "                                  )\n",
    "# Pass through model\n",
    "with torch.no_grad():\n",
    "    sec_model_output = sec_model(**sec_sentences)\n",
    "\n",
    "# Extract the cls embeddings\n",
    "sec_cls_embeddings = sec_model_output.last_hidden_state[:, 0, :].numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Performance (CLS embeddings):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.79      0.82      1588\n",
      "           1       0.78      0.73      0.75      1045\n",
      "           2       0.80      0.85      0.83      2341\n",
      "           3       0.87      0.87      0.87      1824\n",
      "\n",
      "    accuracy                           0.83      6798\n",
      "   macro avg       0.82      0.81      0.82      6798\n",
      "weighted avg       0.83      0.83      0.82      6798\n",
      "\n",
      "Accuracy (CLS embeddings): 0.8252427184466019\n"
     ]
    }
   ],
   "source": [
    "# Use CLS embeddings to train models\n",
    "X_cls_sec = sec_cls_embeddings\n",
    "y = np.array(sector_labels)\n",
    "\n",
    "# Train classification models\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_cls_sec, y)\n",
    "\n",
    "# Predictions\n",
    "log_reg_preds_cls = log_reg.predict(X_cls_sec)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Logistic Regression Performance (CLS embeddings):\")\n",
    "print(classification_report(y, log_reg_preds_cls))\n",
    "print(\"Accuracy (CLS embeddings):\", accuracy_score(y, log_reg_preds_cls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Part 3: Exploring Google’s Chinese BERT [Bonus]**\n",
    "\n",
    "While multilingual BERT (mBERT) is designed to handle multiple languages, using a language-specific BERT model (e.g., Chinese BERT, Arabic BERT, German BERT) often leads to better embeddings and performance for tasks in that particular language. \n",
    "\n",
    "In English, changing the order of certain words may still preserve the meaning (though it may sound unnatural). However, in Chinese, word order is often strictly required for grammatical correctness and meaning preservation. Here we test word order impact on Chinese BERT embeddings. I provide one example below but you can experiment with different sentences.\n",
    "\n",
    "In English: \n",
    "\n",
    "- \"I ate a cake at restaurant yesterday.\" \n",
    "- \"Yesterday at restaurant I ate a cake.\" \n",
    "\n",
    "In Chinese: \n",
    "\n",
    "- \"昨天我在餐厅吃了蛋糕。\"\n",
    "- \"我吃了蛋糕在餐厅昨天。\" (incorrect grammar in Chinese)\n",
    "\n",
    "Use `bert-base-uncased` and `bert-base-chinese` to tokenize and embed these two sentences and compare the **cosine similarity**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity Between Correct & Incorrect Sentence: 0.9130\n"
     ]
    }
   ],
   "source": [
    "# Load Chinese BERT tokenizer & model\n",
    "MODEL_NAME = \"bert-base-chinese\"\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Define sentences (correct & incorrect order)\n",
    "sentences = [\n",
    "    \"昨天我在餐厅吃了蛋糕。\",  \n",
    "    \"我吃了蛋糕在餐厅昨天。\" \n",
    "]\n",
    "\n",
    "# Tokenize input\n",
    "tokenized_inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Pass through model\n",
    "with torch.no_grad():  \n",
    "    outputs = model(**tokenized_inputs)\n",
    "\n",
    "# Extract CLS embeddings\n",
    "cls_embeddings = outputs.last_hidden_state[:, 0, :].numpy()  # Shape: (2, 768)\n",
    "\n",
    "# Compute cosine similarity\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "similarity = cosine_similarity(cls_embeddings[0], cls_embeddings[1])\n",
    "\n",
    "# Print results\n",
    "print(f\"Cosine Similarity Between Correct & Incorrect Sentence: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity Between Correct & Incorrect Sentence: 0.9672\n"
     ]
    }
   ],
   "source": [
    "# Load Chinese BERT tokenizer & model\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Define sentences (correct & incorrect order)\n",
    "sentences = [\n",
    "    \"I ate a cake at restaurant yesterday.\",  \n",
    "    \"Yesterday at restaurant I ate a cake.\"  \n",
    "]\n",
    "\n",
    "# Tokenize input\n",
    "tokenized_inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Pass through model\n",
    "with torch.no_grad():  \n",
    "    outputs = model(**tokenized_inputs)\n",
    "\n",
    "# Extract CLS embeddings\n",
    "cls_embeddings = outputs.last_hidden_state[:, 0, :].numpy()  # Shape: (2, 768)\n",
    "\n",
    "# Compute cosine similarity\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "similarity = cosine_similarity(cls_embeddings[0], cls_embeddings[1])\n",
    "\n",
    "# Print results\n",
    "print(f\"Cosine Similarity Between Correct & Incorrect Sentence: {similarity:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
